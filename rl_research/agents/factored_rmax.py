"""Implementation of a factored R-MAX algorithm with JAX arrays."""

from __future__ import annotations

import math
from typing import Dict, Sequence, Tuple

import jax.numpy as jnp

from minimal_agents.agents.base import TabularAgent, UpdateResult
from minimal_agents.policies import ActionSelectionPolicy, EpsilonGreedyPolicy


class FactoredRMaxAgent(TabularAgent):
    """Model-based R-MAX agent."""

    def __init__(
        self,
        num_states: int,
        num_actions: int,
        *,
        threshold: float,
        state_shape: Sequence[int],
        r_max: float = 1.0,
        m: int = 5,
        discount: float = 0.95,
        seed: int | None = None,
        policy: ActionSelectionPolicy | None = None,
    ) -> None:
        self.threshold = float(threshold)
        self.r_max = float(r_max)
        self.m = int(m)

        if not state_shape:
            raise ValueError("`state_shape` must contain at least one factor.")

        self.state_shape: Tuple[int, ...] = tuple(int(dim) for dim in state_shape)
        if any(dim <= 0 for dim in self.state_shape):
            raise ValueError("`state_shape` must only contain positive integers.")

        if math.prod(self.state_shape) != num_states:
            raise ValueError(
                "`state_shape` must match `num_states`. "
                f"Expected product {math.prod(self.state_shape)}, got {num_states}."
            )

        self._num_factors = len(self.state_shape)

        self._sa_counts = jnp.zeros((num_states, num_actions), dtype=jnp.int32)
        self._reward_sums = jnp.zeros((num_states, num_actions), dtype=jnp.float32)
        self._factor_counts: list[jnp.ndarray] = [
            jnp.zeros((num_states, num_actions, dim), dtype=jnp.int32)
            for dim in self.state_shape
        ]
        self._uniform_transition = jnp.full(
            num_states, 1.0 / float(num_states), dtype=jnp.float32
        )

        super().__init__(
            num_states=num_states,
            num_actions=num_actions,
            discount=discount,
            seed=seed,
            policy=policy,
        )

    # ------------------------------------------------------------------ #
    def _initialise_parameters(self) -> None:
        optimistic_value = self.r_max / (1.0 - self.discount)
        self._q_values = jnp.full(
            (self.num_states, self.num_actions),
            optimistic_value,
            dtype=jnp.float32,
        )
        self._sa_counts = jnp.zeros_like(self._sa_counts)
        self._reward_sums = jnp.zeros_like(self._reward_sums)
        self._factor_counts = [
            jnp.zeros((self.num_states, self.num_actions, dim), dtype=jnp.int32)
            for dim in self.state_shape
        ]
        self._uniform_transition = jnp.full(
            self.num_states, 1.0 / float(self.num_states), dtype=jnp.float32
        )

    def _default_policy(self) -> ActionSelectionPolicy:
        return EpsilonGreedyPolicy(epsilon=0.0)

    # ------------------------------------------------------------------ #
    def _policy_extras(self, obs: int) -> Dict[str, jnp.ndarray]:
        counts_row = self._sa_counts[obs].astype(jnp.float32)
        total = jnp.sum(counts_row) + 1.0
        return {"counts": counts_row, "total": total}

    def is_known(self, obs: int, action: int) -> bool:
        return int(self._sa_counts[obs, action]) >= self.m

    def _reward_estimate(self, obs: int, action: int) -> float:
        count = int(self._sa_counts[obs, action])
        if count == 0:
            return self.r_max
        return float(self._reward_sums[obs, action] / count)

    def _transition_estimate(self, obs: int, action: int) -> jnp.ndarray:
        count = int(self._sa_counts[obs, action])
        if count == 0:
            return self._uniform_transition
        joint = jnp.ones(self.state_shape, dtype=jnp.float32)
        for factor_idx in range(self._num_factors):
            factor_counts = self._factor_counts[factor_idx][obs, action].astype(
                jnp.float32
            )
            total = float(jnp.sum(factor_counts))
            card = self.state_shape[factor_idx]
            if total == 0.0:
                factor_probs = jnp.full((card,), 1.0 / card, dtype=jnp.float32)
            else:
                factor_probs = factor_counts / total

            reshape_dims = [1] * self._num_factors
            reshape_dims[factor_idx] = card
            joint = joint * factor_probs.reshape(reshape_dims)

        return joint.reshape(self.num_states)

    # ------------------------------------------------------------------ #
    def update(
        self,
        obs: int,
        action: int,
        reward: float,
        next_obs: int,
        *,
        terminated: bool = False,
    ) -> UpdateResult:
        del terminated  # R-MAX is episodic but update logic does not depend on terminal flag.

        obs_idx = int(obs)
        action_idx = int(action)
        next_obs_idx = int(next_obs)

        if not self.is_known(obs_idx, action_idx):
            self._sa_counts = self._sa_counts.at[obs_idx, action_idx].add(1)
            self._reward_sums = self._reward_sums.at[obs_idx, action_idx].add(
                float(reward)
            )
            next_factors = self._state_index_to_factors(next_obs_idx)
            for factor_idx, factor_value in enumerate(next_factors):
                self._factor_counts[factor_idx] = self._factor_counts[factor_idx].at[
                    obs_idx, action_idx, factor_value
                ].add(1)

            if self.is_known(obs_idx, action_idx):
                max_steps = int(
                    jnp.ceil(
                        jnp.log(1.0 / (self.threshold * (1.0 - self.discount)))
                        / (1.0 - self.discount)
                    )
                )
                for _ in range(max_steps):
                    old_q_values = self._q_values

                    for state in range(self.num_states):
                        for act in range(self.num_actions):
                            if self.is_known(state, act):
                                r_hat = self._reward_estimate(state, act)
                                t_hat = self._transition_estimate(state, act)
                                next_values = jnp.max(self._q_values, axis=1)
                                updated = r_hat + self.discount * jnp.dot(
                                    t_hat, next_values
                                )
                                self._q_values = self._q_values.at[state, act].set(
                                    float(updated)
                                )
                    
                    if jnp.max(jnp.abs(old_q_values - self._q_values)) < self.threshold:
                        break
        return UpdateResult()

    def _state_index_to_factors(self, state_idx: int) -> Tuple[int, ...]:
        unravelled = jnp.unravel_index(state_idx, self.state_shape, order="C")
        return tuple(int(component) for component in unravelled)
